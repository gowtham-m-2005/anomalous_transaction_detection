{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46cd8947-5724-4763-b369-86e702634c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fdc5ad0-2822-4651-9b36-0719846a8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Staic Analysis of the Data set\n",
    "#Reading the dataset with pandas\n",
    "df = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc2f00b-2052-4051-b363-2f69a1ce5ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset's Shape: (284807, 31)\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Finding the shape of the dataset - (no.of.rows,no.of.columns)\n",
    "print(\"Dataset's Shape:\",df.shape)\n",
    "print(df.head())\n",
    "#class : 1 - fradulent, 0 - non-fradulent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35b341d1-6af2-440c-ab8c-15535ab04ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time      0\n",
      "V1        0\n",
      "V2        0\n",
      "V3        0\n",
      "V4        0\n",
      "V5        0\n",
      "V6        0\n",
      "V7        0\n",
      "V8        0\n",
      "V9        0\n",
      "V10       0\n",
      "V11       0\n",
      "V12       0\n",
      "V13       0\n",
      "V14       0\n",
      "V15       0\n",
      "V16       0\n",
      "V17       0\n",
      "V18       0\n",
      "V19       0\n",
      "V20       0\n",
      "V21       0\n",
      "V22       0\n",
      "V23       0\n",
      "V24       0\n",
      "V25       0\n",
      "V26       0\n",
      "V27       0\n",
      "V28       0\n",
      "Amount    0\n",
      "Class     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Data Cleaning\n",
    "#Identifying the number of missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff8c9e1c-a8f2-4165-8e7e-76058b84c277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Time        V1        V2        V3        V4        V5        V6  \\\n",
      "0 -1.996583 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
      "1 -1.996583  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
      "2 -1.996562 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
      "3 -1.996562 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
      "4 -1.996541 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
      "\n",
      "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
      "0  0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928   \n",
      "1 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
      "2  0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281   \n",
      "3  0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
      "4  0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267   \n",
      "\n",
      "        V25       V26       V27       V28    Amount  Class  \n",
      "0  0.128539 -0.189115  0.133558 -0.021053  0.244964      0  \n",
      "1  0.167170  0.125895 -0.008983  0.014724 -0.342475      0  \n",
      "2 -0.327642 -0.139097 -0.055353 -0.059752  1.160686      0  \n",
      "3  0.647376 -0.221929  0.062723  0.061458  0.140534      0  \n",
      "4 -0.206010  0.502292  0.219422  0.215153 -0.073403      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "#scaling the columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#scaling - 'Amount' column\n",
    "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "#scaling - 'Time' column\n",
    "df['Time'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1, 1))\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18466f1f-428d-4080-9ab5-f5e9c1344c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolation Forest\n",
      "Number of outliers: 2849\n"
     ]
    }
   ],
   "source": [
    "print(\"Isolation Forest\")\n",
    "# Create the Isolation Forest object\n",
    "clf = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(0.01),\n",
    " max_features=1.0, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the data and tag the outliers\n",
    "features = ['Time','Amount']\n",
    "df_features = df[features]\n",
    "\n",
    "clf.fit(df_features)\n",
    "\n",
    "# Get the predictions\n",
    "y_pred = clf.predict(df_features)\n",
    "\n",
    "# counting the outliers\n",
    "no_of_outliers = (y_pred == -1).sum()\n",
    "print(\"Number of outliers:\", no_of_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c541ef92-4d10-43d7-aee4-c45e9a243fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One Class SVM\n",
      "Number of outliers: 2842\n"
     ]
    }
   ],
   "source": [
    "#One Class SVM(Support Vector Machines)\n",
    "# Create the One-class SVM object\n",
    "print(\"\\nOne Class SVM\")\n",
    "clf = OneClassSVM(kernel='rbf', gamma=0.001, nu=0.01)\n",
    "\n",
    "# Fit the data and tag the outliers\n",
    "clf.fit(df_features)\n",
    "\n",
    "# Get the predictions\n",
    "y_pred = clf.predict(df_features)\n",
    "\n",
    "# Counting the no_of_outliers\n",
    "no_of_outliers = (y_pred == -1).sum()\n",
    "print(\"Number of outliers:\", no_of_outliers)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b06c6d6b-98dc-40cb-b852-c576eff2e50b",
   "metadata": {},
   "source": [
    "We chose to go with Isolation Forest since it was faster than SVM and gave us similar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b910300-3a26-4d5e-82e4-7c2268e74478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers found in training dataset : 1994\n",
      "Number of outliers found in testing dataset : 842\n"
     ]
    }
   ],
   "source": [
    "#Evaluation and Model Selection\n",
    "# Define X and y\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess the training and testing data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Detect outliers on the training and testing data\n",
    "outlier_clf = IsolationForest(n_estimators=100, max_samples='auto', contamination=0.01, max_features=1.0, random_state=42)\n",
    "outlier_clf.fit(X_train_scaled)\n",
    "\n",
    "y_train_pred = outlier_clf.predict(X_train_scaled)\n",
    "no_of_outliers = (y_train_pred == -1).sum()\n",
    "print(\"Number of outliers found in training dataset :\", no_of_outliers)\n",
    "\n",
    "y_test_pred = outlier_clf.predict(X_test_scaled)\n",
    "no_of_outliers = (y_test_pred == -1).sum()\n",
    "print(\"Number of outliers found in testing dataset :\", no_of_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a38269c-2a7d-4cdc-951b-0e67ad0f1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from training and testing data\n",
    "X_train_cleaned = X_train_scaled[y_train_pred != -1]\n",
    "y_train_cleaned = y_train[y_train_pred != -1]\n",
    "X_test_cleaned = X_test_scaled[y_test_pred != -1]\n",
    "y_test_cleaned = y_test[y_test_pred != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "798d81d9-5a54-4736-8b8f-c0bb3b2247cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "{'C': 0.1, 'penalty': 'l2'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     84547\n",
      "           1       0.94      0.54      0.68        54\n",
      "\n",
      "    accuracy                           1.00     84601\n",
      "   macro avg       0.97      0.77      0.84     84601\n",
      "weighted avg       1.00      1.00      1.00     84601\n",
      "\n",
      "DecisionTreeClassifier\n",
      "{'criterion': 'entropy', 'max_depth': 3}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     84547\n",
      "           1       0.90      0.65      0.75        54\n",
      "\n",
      "    accuracy                           1.00     84601\n",
      "   macro avg       0.95      0.82      0.88     84601\n",
      "weighted avg       1.00      1.00      1.00     84601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = [LogisticRegression(), DecisionTreeClassifier()]\n",
    "\n",
    "# Create parameter grids for each classifier\n",
    "lr_params = {'penalty': ['l2'], 'C': [0.1, 1, 10]}\n",
    "dt_params = {'criterion': ['gini', 'entropy'], 'max_depth': [3, 5, 7]}\n",
    "rf_params = {'n_estimators': [100, 300, 500], 'max_depth': [3, 5, 7]}\n",
    "knn_params = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}\n",
    "param_grids = [lr_params, dt_params, rf_params, knn_params]\n",
    "\n",
    "# Loop over classifiers and parameter grids to find the best model\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    clf = GridSearchCV(classifier, param_grids[i], cv=5)\n",
    "    clf.fit(X_train_cleaned, y_train_cleaned)\n",
    "    print(classifier.__class__.__name__)\n",
    "    print(clf.best_params_)\n",
    "    y_pred = clf.predict(X_test_cleaned)\n",
    "    print(classification_report(y_test_cleaned, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39892f87-85db-4346-8124-595fef825842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9997281356012341\n",
      "Precision: 0.8974358974358975\n",
      "Recall: 0.6481481481481481\n",
      "F1 Score: 0.7526881720430108\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model's performance\n",
    "acc = accuracy_score(y_test_cleaned, y_pred)\n",
    "prec = precision_score(y_test_cleaned, y_pred)\n",
    "rec = recall_score(y_test_cleaned, y_pred)\n",
    "f1 = f1_score(y_test_cleaned, y_pred)\n",
    "\n",
    "# print the classification metrics\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"Recall: {rec}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e52c8e-83b6-408a-af0c-5246a81d9cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
